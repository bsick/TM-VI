{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from src.vimlts import VIMLTS\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import activations, initializers\n",
    "from tensorflow.keras import backend as K\n",
    "from src import vimlts_utils_keras as VIMLTS_utils\n",
    "# from src.vimlts_keras import DenseVIMLTS\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sns.set(style=\"darkgrid\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load single weight experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Variational parameters init\n",
    "a_init=(0.2,)\n",
    "b_init=(0.3,)\n",
    "alpha_init = (np.log(np.exp(0.2) -1),)\n",
    "beta_init = (1.,)\n",
    "# List to save the variational distribution during training\n",
    "q_history=[]\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "delta_theta_init = (-5,1.,0.8,0.8,0.6,0.4,0.2,0.2,1.,.0)\n",
    "lambda_init=a_init+b_init+delta_theta_init+alpha_init+beta_init\n",
    "num_lambda=np.shape(lambda_init)[0]\n",
    "lambda_tunable = tf.Variable(lambda_init,dtype='float32')\n",
    "# Instance of a variational distribution as VIMLTS\n",
    "q_dist=VIMLTS(np.shape(delta_theta_init)[0])\n",
    "# Update the variational parameter to the variational distribution\n",
    "q_dist.update_lambda_param(lambda_tunable)\n",
    "# Save the distribution to the history\n",
    "q_history.append(q_dist.get_target_dist())\n",
    "for hist in q_history:\n",
    "    plt.plot(hist[1],hist[0])\n",
    "# plt.legend()\n",
    "plt.grid()\n",
    "plt.title(r'$q_{init}$')\n",
    "plt.ylabel(r'$q(w)$')\n",
    "plt.xlabel(r'$w$');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-1.7052318], shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function h_z2w at 0x7f08bb38c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function h_z2w at 0x7f08bb38c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([-1.4543245], shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function h_z2w at 0x7f08bb38c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function h_z2w at 0x7f08bb38c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([-1.11914], shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function h_z2w at 0x7f08bb38c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function h_z2w at 0x7f08bb38c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([-0.78463733], shape=(1,), dtype=float32)\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function h_z2w at 0x7f08bb38c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function h_z2w at 0x7f08bb38c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor([-0.51165223], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w_single = []\n",
    "test_z = np.linspace(-2,2,5, dtype=np.float32)\n",
    "\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "for z_ in test_z:\n",
    "    z_ = tf.Variable(z_, trainable=True)\n",
    "    q_dist=VIMLTS(np.shape(delta_theta_init)[0])\n",
    "    # Update the variational parameter to the variational distribution\n",
    "    q_dist.update_lambda_param(lambda_tunable)\n",
    "    q_dist, w = q_dist.get_target_dist_for_z(z_)\n",
    "    w_single.append(w)\n",
    "    print(w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compare with keras implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "kernel_a = VIMLTS_utils.to_a(tf.reshape(a_init, (1,1,1)))\n",
    "kernel_theta = VIMLTS_utils.to_theta(delta_theta_init)\n",
    "kernel_theta = VIMLTS_utils.to_theta(tf.reshape(delta_theta_init, (-1,1,1)))\n",
    "kernel_theta=tf.transpose(kernel_theta, perm=[1, 2, 0])\n",
    "kernel_alpha = VIMLTS_utils.to_alpha(tf.reshape(alpha_init, (1,1,1)))\n",
    "kernel_alpha = tf.cast(kernel_alpha, dtype=tf.float32)\n",
    "kernel_z_sample = tf.cast(np.linspace(-2,2,5), dtype=tf.float32)\n",
    "kernel_beta_dist = VIMLTS_utils.init_beta_dist(len(delta_theta_init))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function kernel_eval_h_MLT at 0x7f08b99df8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function kernel_eval_h_MLT at 0x7f08b99df8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "tf.Tensor([[[-1.7052319]]], shape=(1, 1, 1), dtype=float32)\n",
      "tf.Tensor([[[-1.4543245]]], shape=(1, 1, 1), dtype=float32)\n",
      "tf.Tensor([[[-1.11914]]], shape=(1, 1, 1), dtype=float32)\n",
      "tf.Tensor([[[-0.7846373]]], shape=(1, 1, 1), dtype=float32)\n",
      "tf.Tensor([[[-0.5116521]]], shape=(1, 1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w_keras = []\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "for z in kernel_z_sample:\n",
    "    kernel_w_sample=VIMLTS_utils.kernel_h_z2w(z=z[None, None],\n",
    "                                              a=kernel_a,\n",
    "                                              b=b_init,\n",
    "                                              theta=kernel_theta,\n",
    "                                              alpha=kernel_alpha,\n",
    "                                              beta=beta_init,\n",
    "                                              beta_dist=kernel_beta_dist)\n",
    "    print(kernel_w_sample)\n",
    "    w_keras.append(kernel_w_sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class VimltsLinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, alpha_w=1., beta_w=0., alpha_z=1., beta_z=0., theta=[1.], num_samples=10):\n",
    "        super().__init__()\n",
    "        self.alpha_w_init = alpha_w\n",
    "        self.beta_w_init = beta_w\n",
    "        self.alpha_z_init = alpha_z\n",
    "        self.beta_z_init = beta_z\n",
    "        self.theta_init = theta\n",
    "        self.units_ = units\n",
    "        self.num_samples_ = num_samples\n",
    "        self.prior_dist_ = tfd.Normal(loc=0., scale=1.)\n",
    "        self.beta_dist = self.init_beta_dist(len(theta))\n",
    "        self.z_dist_ = None\n",
    "\n",
    "    @staticmethod\n",
    "    def init_beta_dist(M):\n",
    "        in1 = []\n",
    "        in2 = []\n",
    "        for i in range(1,M+1):\n",
    "            in1.append(i)\n",
    "            in2.append(M-i+1)\n",
    "        # print(\"Koeffizienten beta_dist:\")\n",
    "        # print(f'in1 = {in1}')\n",
    "        # print(f'in2 = {in2}')\n",
    "        return tfd.Beta(in1,in2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Initialization of the trainable variational parameters, for x (independent of #units) and for bias\n",
    "        \"\"\"\n",
    "\n",
    "        # Kernel\n",
    "        self.z_dist_ = tfd.Normal(loc=tf.zeros((input_shape[1], self.units_)), scale=tf.ones((input_shape[1], self.units_)))\n",
    "        self.alpha_w = self.add_weight(shape=(input_shape[1], self.units_),\n",
    "                                       initializer=tf.constant_initializer(self.alpha_w_init),\n",
    "                                       trainable=True)\n",
    "        self.beta_w = self.add_weight(shape=(input_shape[1], self.units_),\n",
    "                                       initializer=tf.constant_initializer(self.beta_w_init),\n",
    "                                       trainable=True)\n",
    "        self.alpha_z = self.add_weight(shape=(input_shape[1], self.units_),\n",
    "                                       initializer=tf.constant_initializer(self.alpha_z_init),\n",
    "                                       trainable=True)\n",
    "        self.beta_z = self.add_weight(shape=(input_shape[1], self.units_),\n",
    "                                       initializer=tf.constant_initializer(self.beta_z_init),\n",
    "                                       trainable=True)\n",
    "        theta_prime = tf.tile(self.theta_init, [input_shape[1] * self.units_])\n",
    "        theta_prime = tf.reshape(theta_prime, (input_shape[1], self.units_, len(self.theta_init)))\n",
    "        self.theta_prime = tf.Variable(initial_value=theta_prime, trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def f_1(self, z):\n",
    "        \"\"\"\n",
    "\n",
    "        :param z: [#samples x #input x #output]\n",
    "        :return: [#samples x #input x #output]\n",
    "        \"\"\"\n",
    "        z_ = tf.math.multiply(tf.math.softplus(self.alpha_z), z) - self.beta_z\n",
    "        return  tf.math.sigmoid(z_)\n",
    "\n",
    "    def f_2(self, z_):\n",
    "        \"\"\"\n",
    "\n",
    "        :param z_: [#samples x #input x #output]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        theta_p = self.theta_prime  # [#input x #output x M]\n",
    "        theta_p = tf.concat((theta_p[...,0:1], tf.math.softplus(theta_p[...,1:])), axis=-1)\n",
    "\n",
    "        n=theta_p.shape[-1]\n",
    "        tf.ones((n*(n+1) // 2))\n",
    "        m_triangle = tfp.math.fill_triangular(tf.ones(n*(n+1) // 2), upper=True)\n",
    "\n",
    "        theta = theta_p @ m_triangle\n",
    "        fIm=self.beta_dist.prob(z_[...,None]) # to broadcast beta dist [#samples x #input x #output x M]\n",
    "        return tf.math.reduce_mean(fIm * theta, axis=-1)\n",
    "        # return z_\n",
    "\n",
    "    def f_3(self, z_w):\n",
    "        \"\"\"\n",
    "\n",
    "        :type z_w: object\n",
    "        :return: shape [#sample x #input x #output]\n",
    "        \"\"\"\n",
    "        return tf.math.multiply(tf.math.softplus(self.alpha_w), z_w) - self.beta_w\n",
    "\n",
    "    def get_w_dist(self, num=1000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            zz=tf.dtypes.cast(tf.reshape(tf.linspace(-6,6,num), shape=(-1,1,1)), tf.float32)\n",
    "            tape.watch(zz)\n",
    "            w = self.f_3(self.f_2(self.f_1(zz)))\n",
    "            dw_dz = tape.gradient(w, zz)\n",
    "        # tf.reduce_prod(w.shape[1:]) -> undo gradiant adding because of zz broadcasting\n",
    "        dw_dz /= tf.cast(tf.reduce_prod(w.shape[1:]), dtype=tf.float32)\n",
    "        p_z = self.z_dist_.log_prob(zz)\n",
    "        q_w = p_z - tf.math.log(tf.math.abs(dw_dz))\n",
    "        return q_w,w\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: [#batch x #input]\n",
    "        :param kwargs:\n",
    "        :return: [#samples x #batch x #output]\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = self.z_dist_.sample(self.num_samples_)\n",
    "            print(z)\n",
    "            tape.watch(z)\n",
    "            w = self.f_3(self.f_2(self.f_1(z)))\n",
    "            dw_dz = tape.gradient(w, z)\n",
    "        # compute kl divergence\n",
    "        log_p_w = self.prior_dist_.log_prob(w)\n",
    "        # change of variable ==> p(w) = p(z)/|dw/dz|\n",
    "        log_p_z = self.z_dist_.log_prob(z)\n",
    "        # print(\"dw_dz: \", dw_dz)\n",
    "        # log rules ==> log(p(w)) = log(p(z)) - log(|dw/dz|)\n",
    "        log_q_w = log_p_z - tf.math.log(tf.math.abs(dw_dz))\n",
    "        kl =  tf.reduce_sum(tf.reduce_mean(log_q_w,0)) - tf.reduce_sum(tf.reduce_mean(log_p_w,0))\n",
    "        self.add_loss(kl)\n",
    "\n",
    "        return inputs @ w\n",
    "        # return w\n",
    "\n",
    "vimlts_f = VimltsLinear(2, alpha_z=a_init[0], beta_z=b_init[0], alpha_w=alpha_init[0], beta_w=beta_init[0], theta=delta_theta_init)\n",
    "vimlts_f.build(input_shape=(None, 1))\n",
    "z_ = vimlts_f.f_1(tf.reshape(test_z, (-1,1,1)))\n",
    "z_ = vimlts_f.f_2(z_)\n",
    "w_fast = vimlts_f.f_3(z_)\n",
    "w_fast = w_fast.numpy().squeeze()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under pdf:  0.99999565\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(q_history[0][1],q_history[0][0], label=\"tensorflow api\")\n",
    "q, w = vimlts_f.get_w_dist(1000)\n",
    "q = np.exp(q)\n",
    "\n",
    "plt.plot(w.numpy()[:,0,:],q[:,0,:], label=\"fast_keras_api\")\n",
    "plt.legend()\n",
    "print(\"Area under pdf: \",np.trapz(q[:,0,0],w[:,0,0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare if the three implementations.\n",
    "They must have the same h_z_to_w transformation if we use the same \\lambda parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pandas import DataFrame as df\n",
    "df.from_dict(dict(z_sample=test_z,\n",
    "                  w_basic=np.array(w_single).squeeze(),\n",
    "                  w_keras=np.array(w_keras).squeeze(),\n",
    "                  w_keras_fast_first_neuron=w_fast[:,0],\n",
    "                  w_keras_fast_second_neuron=w_fast[:,1],\n",
    "                  ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "   z_sample   w_basic   w_keras  w_keras_fast_first_neuron  \\\n0      -2.0 -1.705232 -1.705232                  -1.705232   \n1      -1.0 -1.454324 -1.454324                  -1.454324   \n2       0.0 -1.119140 -1.119140                  -1.119140   \n3       1.0 -0.784637 -0.784637                  -0.784637   \n4       2.0 -0.511652 -0.511652                  -0.511652   \n\n   w_keras_fast_second_neuron  \n0                   -1.705232  \n1                   -1.454324  \n2                   -1.119140  \n3                   -0.784637  \n4                   -0.511652  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>z_sample</th>\n      <th>w_basic</th>\n      <th>w_keras</th>\n      <th>w_keras_fast_first_neuron</th>\n      <th>w_keras_fast_second_neuron</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-2.0</td>\n      <td>-1.705232</td>\n      <td>-1.705232</td>\n      <td>-1.705232</td>\n      <td>-1.705232</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.0</td>\n      <td>-1.454324</td>\n      <td>-1.454324</td>\n      <td>-1.454324</td>\n      <td>-1.454324</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>-1.119140</td>\n      <td>-1.119140</td>\n      <td>-1.119140</td>\n      <td>-1.119140</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.784637</td>\n      <td>-0.784637</td>\n      <td>-0.784637</td>\n      <td>-0.784637</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-0.511652</td>\n      <td>-0.511652</td>\n      <td>-0.511652</td>\n      <td>-0.511652</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparer KL div"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class DenseVIMLTS(Layer):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 kl_weight,\n",
    "                 init_tilde_a,\n",
    "                 init_b,\n",
    "                 init_tilde_alpha,\n",
    "                 init_beta,\n",
    "                 init_theta,\n",
    "                 num_samples_per_epoch=2,\n",
    "                 activation=None,\n",
    "                 prior_mu=0.,\n",
    "                 prior_sigma=1.,\n",
    "                 kernel_m=10,\n",
    "                 bias_m=10, **kwargs):\n",
    "        self.init_tilde_a = init_tilde_a\n",
    "        self.init_b = init_b\n",
    "        self.init_tilde_alpha = init_tilde_alpha\n",
    "        self.init_beta = init_beta\n",
    "        self.init_theta = init_theta\n",
    "        self.units = units\n",
    "        self.kl_weight = kl_weight\n",
    "        self.activation = activations.get(activation)\n",
    "        self.prior_mu = prior_mu\n",
    "        self.prior_sigma = prior_sigma\n",
    "        self.kernel_m=kernel_m\n",
    "        self.bias_m=bias_m\n",
    "        self.kernel_beta_dist=VIMLTS_utils.init_beta_dist(self.kernel_m)\n",
    "        self.bias_beta_dist=VIMLTS_utils.init_beta_dist(self.bias_m)\n",
    "        self.epsilon=tf.constant(0.001)\n",
    "        self.init_gauss=True\n",
    "        self.num_samples=num_samples_per_epoch\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.units\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Initialization of the trainable variational parameters, for x (independent of #units) and for bias\n",
    "        \"\"\"\n",
    "\n",
    "        # Kernel\n",
    "        self.kernel_tilde_a = self.add_weight(name='kernel_tilde_a',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                             initializer=initializers.Constant(self.init_tilde_a),\n",
    "                                          trainable=True)\n",
    "        self.kernel_b = self.add_weight(name='kernel_b',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                     initializer=initializers.Constant(self.init_b),\n",
    "                                          trainable=True)\n",
    "\n",
    "        self.kernel_start_theta = self.add_weight(name='kernel_start_theta',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[0]),\n",
    "                                          trainable=True)\n",
    "        self.kernel_delta_theta_1 = self.add_weight(name='kernel_delta_theta_1',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[1]),\n",
    "                                          trainable=True)\n",
    "        self.kernel_delta_theta_2 = self.add_weight(name='kernel_delta_theta_2',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[2]),\n",
    "                                          trainable=True)\n",
    "        self.kernel_delta_theta_3 = self.add_weight(name='kernel_delta_theta_3',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[3]),\n",
    "                                          trainable=True)\n",
    "        self.kernel_delta_theta_4 = self.add_weight(name='kernel_delta_theta_4',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[4]),\n",
    "                                          trainable=True)\n",
    "        self.kernel_delta_theta_5 = self.add_weight(name='kernel_delta_theta_5',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[5]),\n",
    "                                          trainable=True)\n",
    "        self.kernel_delta_theta_6 = self.add_weight(name='kernel_delta_theta_6',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[6]),\n",
    "                                          trainable=True)\n",
    "        self.kernel_delta_theta_7 = self.add_weight(name='kernel_delta_theta_7',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[7]),\n",
    "                                          trainable=True)\n",
    "        self.kernel_delta_theta_8 = self.add_weight(name='kernel_delta_theta_8',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[8]),\n",
    "                                          trainable=True)\n",
    "        self.kernel_delta_theta_9 = self.add_weight(name='kernel_delta_theta_9',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[9]),\n",
    "                                          trainable=True)\n",
    "\n",
    "        self.kernel_tilde_alpha = self.add_weight(name='kernel_tilde_alpha',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                                 initializer=initializers.Constant(self.init_tilde_alpha),\n",
    "                                          trainable=True)\n",
    "        self.kernel_beta = self.add_weight(name='kernel_beta',\n",
    "                                          shape=(input_shape[1], self.units),\n",
    "                                         initializer=initializers.Constant(self.init_beta),\n",
    "                                          trainable=True)\n",
    "\n",
    "\n",
    "        # Bias\n",
    "        self.bias_tilde_a = self.add_weight(name='bias_tilde_a',\n",
    "                                          shape=(self.units,),\n",
    "                                         initializer=initializers.Constant(self.init_tilde_a),\n",
    "                                          trainable=True)\n",
    "        self.bias_b = self.add_weight(name='bias_b',\n",
    "                                          shape=(self.units,),\n",
    "                                     initializer=initializers.Constant(self.init_b),\n",
    "                                          trainable=True)\n",
    "\n",
    "        self.bias_start_theta = self.add_weight(name='bias_start_theta',\n",
    "                                          shape=(self.units,),\n",
    "                                             initializer=initializers.Constant(self.init_theta[0]),\n",
    "                                          trainable=True)\n",
    "        self.bias_delta_theta_1 = self.add_weight(name='bias_delta_theta_1',\n",
    "                                          shape=(self.units,),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[1]),\n",
    "                                          trainable=True)\n",
    "        self.bias_delta_theta_2 = self.add_weight(name='bias_delta_theta_2',\n",
    "                                          shape=(self.units,),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[2]),\n",
    "                                          trainable=True)\n",
    "        self.bias_delta_theta_3 = self.add_weight(name='bias_delta_theta_3',\n",
    "                                          shape=(self.units,),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[3]),\n",
    "                                          trainable=True)\n",
    "        self.bias_delta_theta_4 = self.add_weight(name='bias_delta_theta_4',\n",
    "                                          shape=(self.units,),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[4]),\n",
    "                                          trainable=True)\n",
    "        self.bias_delta_theta_5 = self.add_weight(name='bias_delta_theta_5',\n",
    "                                          shape=(self.units,),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[5]),\n",
    "                                          trainable=True)\n",
    "        self.bias_delta_theta_6 = self.add_weight(name='bias_delta_theta_6',\n",
    "                                          shape=(self.units,),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[6]),\n",
    "                                          trainable=True)\n",
    "        self.bias_delta_theta_7 = self.add_weight(name='bias_delta_theta_7',\n",
    "                                          shape=(self.units,),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[7]),\n",
    "                                          trainable=True)\n",
    "        self.bias_delta_theta_8 = self.add_weight(name='bias_delta_theta_8',\n",
    "                                          shape=(self.units,),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[8]),\n",
    "                                          trainable=True)\n",
    "        self.bias_delta_theta_9 = self.add_weight(name='bias_delta_theta_9',\n",
    "                                          shape=(self.units,),\n",
    "                                                 initializer=initializers.Constant(self.init_theta[9]),\n",
    "                                          trainable=True)\n",
    "\n",
    "        self.bias_tilde_alpha = self.add_weight(name='bias_tilde_alpha',\n",
    "                                          shape=(self.units,),\n",
    "                                             initializer=initializers.Constant(self.init_tilde_alpha),\n",
    "                                          trainable=True)\n",
    "        self.bias_beta = self.add_weight(name='bias_beta',\n",
    "                                          shape=(self.units,),\n",
    "                                         initializer=initializers.Constant(self.init_beta),\n",
    "                                          trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"Calculation of the forward direction of the network by\n",
    "        -drawing parameters\n",
    "        -Calculation loss KL\n",
    "        -Calculation output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        # 1.) Conversion of trainable variational parameters to parameters of distribution (x)\n",
    "        kernel_a = VIMLTS_utils.to_a(self.kernel_tilde_a)\n",
    "        kernel_theta = VIMLTS_utils.to_theta([self.kernel_start_theta,\n",
    "                                        self.kernel_delta_theta_1,\n",
    "                                        self.kernel_delta_theta_2,\n",
    "                                        self.kernel_delta_theta_3,\n",
    "                                        self.kernel_delta_theta_4,\n",
    "                                        self.kernel_delta_theta_5,\n",
    "                                        self.kernel_delta_theta_6,\n",
    "                                        self.kernel_delta_theta_7,\n",
    "                                        self.kernel_delta_theta_8,\n",
    "                                        self.kernel_delta_theta_9])\n",
    "        kernel_theta=tf.transpose(kernel_theta, perm=[1, 2, 0])\n",
    "        kernel_alpha = VIMLTS_utils.to_alpha(self.kernel_tilde_alpha)\n",
    "\n",
    "        # 3.) Conversion of trainable variational parameters to parameters of distribution (bias)\n",
    "        bias_a = VIMLTS_utils.to_a(self.bias_tilde_a)\n",
    "        bias_theta = VIMLTS_utils.to_theta([self.bias_start_theta,\n",
    "                                        self.bias_delta_theta_1,\n",
    "                                        self.bias_delta_theta_2,\n",
    "                                        self.bias_delta_theta_3,\n",
    "                                        self.bias_delta_theta_4,\n",
    "                                        self.bias_delta_theta_5,\n",
    "                                        self.bias_delta_theta_6,\n",
    "                                        self.bias_delta_theta_7,\n",
    "                                        self.bias_delta_theta_8,\n",
    "                                        self.bias_delta_theta_9])\n",
    "        bias_theta=tf.transpose(bias_theta, perm=[1, 0])\n",
    "        bias_alpha = VIMLTS_utils.to_alpha(self.bias_tilde_alpha)\n",
    "\n",
    "\n",
    "        # 2.) Sampling from variational dist by reparametrization trick - prototype version (x)\n",
    "        samples = tf.random.normal((self.num_samples,)+self.kernel_tilde_a.shape)\n",
    "        for current_sample, kernel_z_sample in enumerate(samples):\n",
    "            # kernel_z_sample=tf.random.normal(self.kernel_tilde_a.shape)\n",
    "            print(\"keras z sample: \", kernel_z_sample)\n",
    "            kernel_w_sample=VIMLTS_utils.kernel_h_z2w(z=kernel_z_sample,\n",
    "                                            a=kernel_a,\n",
    "                                            b=self.kernel_b,\n",
    "                                            theta=kernel_theta,\n",
    "                                            alpha=kernel_alpha,\n",
    "                                            beta=self.kernel_beta,\n",
    "                                            beta_dist=self.kernel_beta_dist)\n",
    "\n",
    "            if current_sample==0:\n",
    "                kernel_w_sample_list=tf.reshape(kernel_w_sample,[1,kernel_w_sample.shape[0],kernel_w_sample.shape[1]])\n",
    "            else:\n",
    "                kernel_w_sample_list=tf.concat([kernel_w_sample_list,tf.reshape(kernel_w_sample,[1,kernel_w_sample.shape[0],kernel_w_sample.shape[1]])],axis=0)\n",
    "\n",
    "            print(\"keras w sample: \", kernel_w_sample)\n",
    "\n",
    "            # 4.) Sampling from variational dist by reparametrization trick - prototype version (bias)\n",
    "            bias_z_sample=tf.random.normal(self.bias_tilde_a.shape)\n",
    "            bias_w_sample=VIMLTS_utils.bias_h_z2w(z=bias_z_sample,\n",
    "                                            a=bias_a,\n",
    "                                            b=self.bias_b,\n",
    "                                            theta=bias_theta,\n",
    "                                            alpha=bias_alpha,\n",
    "                                            beta=self.bias_beta,\n",
    "                                            beta_dist=self.bias_beta_dist)\n",
    "            if current_sample==0:\n",
    "                bias_w_sample_list=tf.reshape(bias_w_sample,[1,bias_w_sample.shape[0]])\n",
    "            else:\n",
    "                bias_w_sample_list=tf.concat([bias_w_sample_list,tf.reshape(bias_w_sample,[1,bias_w_sample.shape[0]])],axis=0)\n",
    "\n",
    "\n",
    "            # 5.) Calculation l_kl by calling function with passing the sample and the variational parameters (x and bias)\n",
    "\n",
    "            kernel_kl_loss=self.calc_kernel_kl_loss(kernel_w_sample=kernel_w_sample,\n",
    "                                kernel_z_sample=kernel_z_sample,\n",
    "                                kernel_a=kernel_a,\n",
    "                                kernel_theta=kernel_theta,\n",
    "                                kernel_alpha=kernel_alpha)\n",
    "            if current_sample==0:\n",
    "                kernel_kl_loss_sum=kernel_kl_loss\n",
    "            else:\n",
    "                kernel_kl_loss_sum+=kernel_kl_loss\n",
    "\n",
    "\n",
    "            bias_kl_loss=self.calc_bias_kl_loss(bias_w_sample=bias_w_sample,\n",
    "                                bias_z_sample=bias_z_sample,\n",
    "                                bias_a=bias_a,\n",
    "                                bias_theta=bias_theta,\n",
    "                                bias_alpha=bias_alpha)\n",
    "            if current_sample==0:\n",
    "                bias_kl_loss_sum=bias_kl_loss\n",
    "            else:\n",
    "                bias_kl_loss_sum+=bias_kl_loss\n",
    "\n",
    "\n",
    "        # self.add_loss(1/self.num_samples*kernel_kl_loss_sum+1/self.num_samples*bias_kl_loss_sum)\n",
    "        self.add_loss(1/self.num_samples*kernel_kl_loss_sum)\n",
    "\n",
    "        kernel_mean=tf.reduce_mean(kernel_w_sample_list,axis=0)\n",
    "        bias_mean=tf.reduce_mean(bias_w_sample_list,axis=0)\n",
    "\n",
    "        # 6.) Calculation output of the layer\n",
    "        # return self.activation(K.dot(inputs, kernel_mean)+ bias_mean)\n",
    "        return K.dot(inputs, kernel_mean)\n",
    "\n",
    "    def calc_kernel_kl_loss(self,kernel_w_sample,kernel_z_sample,kernel_a,kernel_theta,kernel_alpha):\n",
    "        kernel_z_epsilon=kernel_z_sample+self.epsilon\n",
    "        kernel_q,w_check=VIMLTS_utils.kernel_eval_variational_dist(z=kernel_z_sample,\n",
    "                                                            z_epsilon=kernel_z_epsilon,\n",
    "                                                            a=kernel_a,\n",
    "                                                            b=self.kernel_b,\n",
    "                                                            theta=kernel_theta,\n",
    "                                                            alpha=kernel_alpha,\n",
    "                                                            beta=self.kernel_beta,\n",
    "                                                            beta_dist=self.kernel_beta_dist)\n",
    "        kernel_kl_loss=self.kl_loss(kernel_q,kernel_w_sample)\n",
    "        return kernel_kl_loss\n",
    "\n",
    "    def calc_bias_kl_loss(self,bias_w_sample,bias_z_sample,bias_a,bias_theta,bias_alpha):\n",
    "        bias_z_epsilon=bias_z_sample+self.epsilon\n",
    "        bias_q,w_check=VIMLTS_utils.bias_eval_variational_dist(z=bias_z_sample,\n",
    "                                                            z_epsilon=bias_z_epsilon,\n",
    "                                                            a=bias_a,\n",
    "                                                            b=self.bias_b,\n",
    "                                                            theta=bias_theta,\n",
    "                                                            alpha=bias_alpha,\n",
    "                                                            beta=self.bias_beta,\n",
    "                                                            beta_dist=self.bias_beta_dist)\n",
    "        bias_kl_loss=self.kl_loss(bias_q,bias_w_sample)\n",
    "        return bias_kl_loss\n",
    "\n",
    "\n",
    "    def kl_loss(self, q, w_sample):\n",
    "        # Calculate KL divergence between variational dist and prior\n",
    "        return self.kl_weight * K.sum(K.log(q) - self.log_prior_prob(w_sample))\n",
    "\n",
    "\n",
    "    def log_prior_prob(self, w):\n",
    "        # Prior\n",
    "        prior_dist = tfp.distributions.Normal(self.prior_mu, self.prior_sigma)\n",
    "        return prior_dist.log_prob(w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"vimlts_linear_1/vimlts_linear_1_Normal/sample/Reshape:0\", shape=(3, 2, 10), dtype=float32)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method VimltsLinear.f_2 of <__main__.VimltsLinear object at 0x7f0824592fd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method VimltsLinear.f_2 of <__main__.VimltsLinear object at 0x7f0824592fd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Tensor(\"vimlts_linear_1/vimlts_linear_1_Normal/sample/Reshape:0\", shape=(3, 2, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[-1.1012203   1.5457517   0.383644   -0.87965786 -1.2246722\n",
      "   -0.9811211   0.08780783 -0.20326038 -0.5581562  -0.7205441 ]\n",
      "  [-0.6259924  -0.71502596 -0.34835446 -0.33646983  0.18257578\n",
      "    1.1085953   1.2796588  -0.02147583 -0.31968883  0.37332553]]\n",
      "\n",
      " [[ 0.25279108  0.6437664   2.146308   -0.8251497  -0.9041368\n",
      "    1.3948786   1.2248751   0.05864959 -0.49213138 -0.81997806]\n",
      "  [-0.18526012 -0.39277685 -0.65852255 -0.9833388   0.38883775\n",
      "   -1.0372449  -1.5600569  -0.15791254 -0.3566943  -0.2004403 ]]\n",
      "\n",
      " [[ 1.613107    0.6796728   0.08133233  1.3380764   1.1848053\n",
      "   -0.35381562 -0.10400175 -0.75114644 -0.3727463  -0.8732732 ]\n",
      "  [ 0.1690602   1.0454441   0.92061925 -0.36889285  0.10171467\n",
      "   -1.1921712  -1.4274356   1.9076501  -0.7215866   1.2887349 ]]], shape=(3, 2, 10), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\narray([[-2.2342231, -1.9341811, -1.98382  , -2.4782667, -2.2655249,\n        -2.3622553, -2.2815022, -2.1665604, -2.564245 , -2.3522317],\n       [-2.2342231, -1.9341811, -1.98382  , -2.4782667, -2.2655249,\n        -2.3622553, -2.2815022, -2.1665604, -2.564245 , -2.3522317],\n       [-2.2342231, -1.9341811, -1.98382  , -2.4782667, -2.2655249,\n        -2.3622553, -2.2815022, -2.1665604, -2.564245 , -2.3522317],\n       [-2.2342231, -1.9341811, -1.98382  , -2.4782667, -2.2655249,\n        -2.3622553, -2.2815022, -2.1665604, -2.564245 , -2.3522317],\n       [-2.2342231, -1.9341811, -1.98382  , -2.4782667, -2.2655249,\n        -2.3622553, -2.2815022, -2.1665604, -2.564245 , -2.3522317]],\n      dtype=float32)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vimlts_f = VimltsLinear(10, num_samples=3, alpha_z=a_init[0], beta_z=b_init[0], alpha_w=alpha_init[0], beta_w=beta_init[0], theta=delta_theta_init)\n",
    "model_f = tf.keras.Sequential(vimlts_f, name='vimlts')\n",
    "model_f.build(input_shape=(None, 2))\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "out_f = model_f(tf.ones((5,2)))\n",
    "# model_f.fit(tf.ones((5,1)), tf.ones((5,1))*4)\n",
    "tf.reduce_mean(out_f, 0)  # ToDo: Aber reduce mean ist eigentlich nicht was wir hier machen wollen"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras z sample:  tf.Tensor(\n",
      "[[-1.1012203   1.5457517   0.383644   -0.87965786 -1.2246722  -0.9811211\n",
      "   0.08780783 -0.20326038 -0.5581562  -0.7205441 ]\n",
      " [-0.6259924  -0.71502596 -0.34835446 -0.33646983  0.18257578  1.1085953\n",
      "   1.2796588  -0.02147583 -0.31968883  0.37332553]], shape=(2, 10), dtype=float32)\n",
      "keras w sample:  tf.Tensor(\n",
      "[[-1.4845272  -0.6260103  -0.98599577 -1.4171896  -1.5199972  -1.4485846\n",
      "  -1.088407   -1.1903173  -1.3124056  -1.3662441 ]\n",
      " [-1.3350978  -1.3644431  -1.2407627  -1.2366518  -1.0553596  -0.7514379\n",
      "  -0.7007315  -1.1266654  -1.23084    -0.9895232 ]], shape=(2, 10), dtype=float32)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function bias_eval_h_MLT at 0x7f08b99df940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function bias_eval_h_MLT at 0x7f08b99df940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "keras z sample:  tf.Tensor(\n",
      "[[ 0.25279108  0.6437664   2.146308   -0.8251497  -0.9041368   1.3948786\n",
      "   1.2248751   0.05864959 -0.49213138 -0.81997806]\n",
      " [-0.18526012 -0.39277685 -0.65852255 -0.9833388   0.38883775 -1.0372449\n",
      "  -1.5600569  -0.15791254 -0.3566943  -0.2004403 ]], shape=(2, 10), dtype=float32)\n",
      "keras w sample:  tf.Tensor(\n",
      "[[-1.0309999  -0.89858127 -0.47877192 -1.399961   -1.4248462  -0.66773224\n",
      "  -0.71675247 -1.0986038  -1.2900752  -1.3983138 ]\n",
      " [-1.184028   -1.2560858  -1.3458803  -1.4492606  -0.9842218  -1.4655514\n",
      "  -1.6082286  -1.174464   -1.2436447  -1.1893322 ]], shape=(2, 10), dtype=float32)\n",
      "keras z sample:  tf.Tensor(\n",
      "[[ 1.613107    0.6796728   0.08133233  1.3380764   1.1848053  -0.35381562\n",
      "  -0.10400175 -0.75114644 -0.3727463  -0.8732732 ]\n",
      " [ 0.1690602   1.0454441   0.92061925 -0.36889285  0.10171467 -1.1921712\n",
      "  -1.4274356   1.9076501  -0.7215866   1.2887349 ]], shape=(2, 10), dtype=float32)\n",
      "keras w sample:  tf.Tensor(\n",
      "[[-0.60795474 -0.88677126 -1.0906707  -0.6838809  -0.7286018  -1.2426503\n",
      "  -1.1555874  -1.3761928  -1.249185   -1.4151843 ]\n",
      " [-1.060062   -0.7706517  -0.80937856 -1.2478558  -1.0835477  -1.5108094\n",
      "  -1.5747998  -0.5334382  -1.3665842  -0.6980976 ]], shape=(2, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.executing_eagerly()\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "layer = DenseVIMLTS(units=10,\n",
    "                    num_samples_per_epoch=3,\n",
    "                        kl_weight=1.,\n",
    "                        init_tilde_a = a_init[0],\n",
    "                        init_b = b_init[0],\n",
    "                        init_tilde_alpha = alpha_init[0],\n",
    "                        init_beta = beta_init[0],\n",
    "                        init_theta = delta_theta_init)\n",
    "layer.build(input_shape=(None, 2))\n",
    "out_keras = layer(tf.ones((5,2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare output again\n",
    "Use the complete Keras code from Stefan with some little changes to compare the two implementations.\n",
    "Changes are:\n",
    "    - same lambda initialization\n",
    "    - same sample order (z-space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    },
    {
     "data": {
      "text/plain": "   out_fast  out_keras\n0 -2.234223  -2.234223\n1 -1.934181  -1.934181\n2 -1.983820  -1.983820\n3 -2.478267  -2.478266\n4 -2.265525  -2.265525\n5 -2.362255  -2.362255\n6 -2.281502  -2.281502\n7 -2.166560  -2.166560\n8 -2.564245  -2.564245\n9 -2.352232  -2.352232",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>out_fast</th>\n      <th>out_keras</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-2.234223</td>\n      <td>-2.234223</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.934181</td>\n      <td>-1.934181</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.983820</td>\n      <td>-1.983820</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-2.478267</td>\n      <td>-2.478266</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-2.265525</td>\n      <td>-2.265525</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-2.362255</td>\n      <td>-2.362255</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-2.281502</td>\n      <td>-2.281502</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-2.166560</td>\n      <td>-2.166560</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-2.564245</td>\n      <td>-2.564245</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-2.352232</td>\n      <td>-2.352232</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test Passed\") if np.isclose(out_keras, tf.reduce_mean(out_f, 0)).all() else print(\"Test Failed\")\n",
    "df.from_dict(dict(out_fast=tf.reduce_mean(out_f, 0).numpy().squeeze()[0],\n",
    "                  out_keras=out_keras.numpy().squeeze()[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare KL divergence computation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Failed\n"
     ]
    },
    {
     "data": {
      "text/plain": "   vimlts_fast_kl_with_grad  vimlts_keras_kl_with_eps\n0                 28.637932                 28.636328",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vimlts_fast_kl_with_grad</th>\n      <th>vimlts_keras_kl_with_eps</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28.637932</td>\n      <td>28.636328</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test Passed\") if np.isclose(layer.losses[0], vimlts_f.losses[0]).all() else print(\"Test Failed\")\n",
    "df.from_dict(dict(vimlts_fast_kl_with_grad=vimlts_f.losses[0].numpy().reshape(-1),\n",
    "                  vimlts_keras_kl_with_eps=layer.losses[0].numpy().reshape(-1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method VimltsLinear.f_2 of <src.vimlts_fast.VimltsLinear object at 0x7f07fe52ab80>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method VimltsLinear.f_2 of <src.vimlts_fast.VimltsLinear object at 0x7f07fe52ab80>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "from src.vimlts_fast import VimltsLinear\n",
    "initializers_kernel = dict(kernel_init_alpha_w = initializers.Constant(alpha_init[0]),\n",
    "                    kernel_init_beta_w = initializers.Constant(beta_init[0]),\n",
    "                    kernel_init_alpha_z = initializers.Constant(a_init[0]),\n",
    "                    kernel_init_beta_z = initializers.Constant(b_init[0]),\n",
    "                    kernel_init_thetas = [initializers.Constant(i) for i in delta_theta_init])\n",
    "vimlts_ff = VimltsLinear(10, activation=lambda x:x,\n",
    "                        num_samples=3, **initializers_kernel)\n",
    "model_ff = tf.keras.Sequential(vimlts_ff, name='vimlts')\n",
    "model_ff.build(input_shape=(None, 2))\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "# x = tf.convert_to_tensor(np.arange(0,5,5, dtype=np.float32).reshape(-1,1))\n",
    "# out_ff = model_ff(x)\n",
    "out_ff = model_ff(tf.ones((5,2)))\n",
    "# model_f.fit(tf.ones((5,1)), tf.ones((5,1))*4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The algo was ported into an own module.\n",
    "Now we compare it again"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    },
    {
     "data": {
      "text/plain": "   vimlts_fast_kl_with_grad  vimlts_fast_bib_kl_with_grad  \\\n0                 28.637932                     28.637932   \n\n   vimlts_keras_kl_with_eps  \n0                 28.636328  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vimlts_fast_kl_with_grad</th>\n      <th>vimlts_fast_bib_kl_with_grad</th>\n      <th>vimlts_keras_kl_with_eps</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28.637932</td>\n      <td>28.637932</td>\n      <td>28.636328</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test Passed\") if np.isclose(vimlts_ff.losses[0], vimlts_f.losses[0]).all() else print(\"Test Failed\")\n",
    "df.from_dict(dict(vimlts_fast_kl_with_grad=vimlts_f.losses[0].numpy().reshape(-1),\n",
    "                  vimlts_fast_bib_kl_with_grad=vimlts_ff.losses[0].numpy().reshape(-1),\n",
    "                  vimlts_keras_kl_with_eps=layer.losses[0].numpy().reshape(-1)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed\n"
     ]
    },
    {
     "data": {
      "text/plain": "   out_fast  out_fast_bib  out_keras\n0 -2.234223     -2.234223  -2.234223\n1 -1.934181     -1.934181  -1.934181\n2 -1.983820     -1.983820  -1.983820\n3 -2.478267     -2.478267  -2.478266\n4 -2.265525     -2.265525  -2.265525\n5 -2.362255     -2.362255  -2.362255\n6 -2.281502     -2.281502  -2.281502\n7 -2.166560     -2.166560  -2.166560\n8 -2.564245     -2.564245  -2.564245\n9 -2.352232     -2.352232  -2.352232",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>out_fast</th>\n      <th>out_fast_bib</th>\n      <th>out_keras</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-2.234223</td>\n      <td>-2.234223</td>\n      <td>-2.234223</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.934181</td>\n      <td>-1.934181</td>\n      <td>-1.934181</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.983820</td>\n      <td>-1.983820</td>\n      <td>-1.983820</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-2.478267</td>\n      <td>-2.478267</td>\n      <td>-2.478266</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-2.265525</td>\n      <td>-2.265525</td>\n      <td>-2.265525</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-2.362255</td>\n      <td>-2.362255</td>\n      <td>-2.362255</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-2.281502</td>\n      <td>-2.281502</td>\n      <td>-2.281502</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-2.166560</td>\n      <td>-2.166560</td>\n      <td>-2.166560</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-2.564245</td>\n      <td>-2.564245</td>\n      <td>-2.564245</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-2.352232</td>\n      <td>-2.352232</td>\n      <td>-2.352232</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test Passed\") if np.isclose(out_ff, out_f).all() else print(\"Test Failed\")\n",
    "df.from_dict(dict(out_fast=tf.reduce_mean(out_f, 0).numpy().squeeze()[0],\n",
    "                  out_fast_bib=tf.reduce_mean(out_ff, 0).numpy().squeeze()[0],\n",
    "                  out_keras=out_keras.numpy().squeeze()[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1000,2,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-1c74c12cddd4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mq_pdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvimlts_ff\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_w_dist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mq_pdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtitle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Area under pdf of first neuron: {np.trapz(q_pdf[:,0,0], w[:,0,0])}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Modeling-of-flexible-Posterior-Distributions-for-Bayesian-Neural-Networks/src/vimlts_fast.py\u001B[0m in \u001B[0;36mget_w_dist\u001B[0;34m(self, num)\u001B[0m\n\u001B[1;32m    187\u001B[0m             \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mzz\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    188\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mactivate_kernel_transformation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 189\u001B[0;31m             \u001B[0mw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf_3\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mzz\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    190\u001B[0m             \u001B[0mdw_dz\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mzz\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    191\u001B[0m         \u001B[0;31m# tf.reduce_prod(w.shape[1:]) -> undo gradiant adding because of zz broadcasting\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Modeling-of-flexible-Posterior-Distributions-for-Bayesian-Neural-Networks/src/vimlts_fast.py\u001B[0m in \u001B[0;36mf_2\u001B[0;34m(self, z_)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m         \u001B[0mtheta\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtheta_p\u001B[0m \u001B[0;34m@\u001B[0m \u001B[0mm_triangle\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 172\u001B[0;31m         \u001B[0mfIm\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbeta_dist\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mz_\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m...\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# to broadcast beta dist [#samples x #input x #output x M]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    173\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduce_mean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfIm\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mtheta\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m         \u001B[0;31m# return z_\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py\u001B[0m in \u001B[0;36mprob\u001B[0;34m(self, value, name, **kwargs)\u001B[0m\n\u001B[1;32m   1056\u001B[0m         \u001B[0mvalues\u001B[0m \u001B[0mof\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1057\u001B[0m     \"\"\"\n\u001B[0;32m-> 1058\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1059\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1060\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_call_log_cdf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py\u001B[0m in \u001B[0;36m_call_prob\u001B[0;34m(self, value, name, **kwargs)\u001B[0m\n\u001B[1;32m   1038\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name_and_control_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1039\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'_prob'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1040\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1041\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'_log_prob'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_log_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow_probability/python/internal/distribution_util.py\u001B[0m in \u001B[0;36m_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1362\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mfunctools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwraps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1363\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1364\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1365\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1366\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0m_fn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow_probability/python/distributions/beta.py\u001B[0m in \u001B[0;36m_prob\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    264\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mdistribution_util\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAppendDocstring\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_beta_sample_note\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    265\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 266\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_log_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    267\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    268\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mdistribution_util\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAppendDocstring\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_beta_sample_note\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow_probability/python/internal/distribution_util.py\u001B[0m in \u001B[0;36m_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1362\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mfunctools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwraps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1363\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1364\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1365\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1366\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0m_fn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow_probability/python/distributions/beta.py\u001B[0m in \u001B[0;36m_log_prob\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    259\u001B[0m     \u001B[0mconcentration0\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcentration0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    260\u001B[0m     \u001B[0mconcentration1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcentration1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 261\u001B[0;31m     return (self._log_unnormalized_prob(x, concentration1, concentration0) -\n\u001B[0m\u001B[1;32m    262\u001B[0m             self._log_normalization(concentration1, concentration0))\n\u001B[1;32m    263\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow_probability/python/distributions/beta.py\u001B[0m in \u001B[0;36m_log_unnormalized_prob\u001B[0;34m(self, x, concentration1, concentration0)\u001B[0m\n\u001B[1;32m    283\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    284\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_log_unnormalized_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconcentration1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconcentration0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 285\u001B[0;31m     return (tf.math.xlogy(concentration1 - 1., x) +\n\u001B[0m\u001B[1;32m    286\u001B[0m             tf.math.xlog1py(concentration0 - 1., -x))\n\u001B[1;32m    287\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mbinary_op_wrapper\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m   1162\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mop_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1163\u001B[0m       \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1164\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1165\u001B[0m       \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1166\u001B[0m         \u001B[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    199\u001B[0m     \u001B[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 201\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    202\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m       \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36m_add_dispatch\u001B[0;34m(x, y, name)\u001B[0m\n\u001B[1;32m   1484\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1485\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1486\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_v2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1487\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1488\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36madd_v2\u001B[0;34m(x, y, name)\u001B[0m\n\u001B[1;32m    470\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    471\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 472\u001B[0;31m       \u001B[0m_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    473\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    474\u001B[0m       \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mraise_from_not_ok_status\u001B[0;34m(e, name)\u001B[0m\n\u001B[1;32m   6860\u001B[0m   \u001B[0mmessage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmessage\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\" name: \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6861\u001B[0m   \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6862\u001B[0;31m   \u001B[0msix\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_status_to_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6863\u001B[0m   \u001B[0;31m# pylint: enable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6864\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/six.py\u001B[0m in \u001B[0;36mraise_from\u001B[0;34m(value, from_value)\u001B[0m\n",
      "\u001B[0;31mResourceExhaustedError\u001B[0m: OOM when allocating tensor with shape[1000,2,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "q_pdf, w = vimlts_ff.get_w_dist()\n",
    "plt.plot(w[:,0,0], q_pdf[:,0,0])\n",
    "plt.title(f\"Area under pdf of first neuron: {np.trapz(q_pdf[:,0,0], w[:,0,0])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check new VIMLTS implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 2, 4), dtype=float32, numpy=\narray([[[-3.0045245, -2.074595 , -2.0744028, -2.6075068],\n        [-3.0045245, -2.074595 , -2.0744028, -2.6075068]],\n\n       [[-2.5531683, -2.6028957, -2.3904574, -2.115881 ],\n        [-2.5531683, -2.6028957, -2.3904574, -2.115881 ]],\n\n       [[-1.7317314, -2.0252466, -1.7096119, -2.3894842],\n        [-1.7317314, -2.0252466, -1.7096119, -2.3894842]]], dtype=float32)>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.vimlts_fast import VimltsLinear\n",
    "initializers_kernel = dict(kernel_init_alpha_w = initializers.Constant(alpha_init[0]),\n",
    "                    kernel_init_beta_w = initializers.Constant(beta_init[0]),\n",
    "                    kernel_init_alpha_z = initializers.Constant(a_init[0]),\n",
    "                    kernel_init_beta_z = initializers.Constant(b_init[0]),\n",
    "                    kernel_init_thetas = [initializers.Constant(i) for i in delta_theta_init])\n",
    "initializers_bias = dict(bias_init_alpha_w = initializers.RandomNormal(),\n",
    "                         bias_init_beta_w = initializers.RandomNormal(),\n",
    "                         bias_init_alpha_z = initializers.RandomNormal(),\n",
    "                         bias_init_beta_z = initializers.RandomNormal(),\n",
    "                         bias_init_thetas = [initializers.RandomNormal(i) for i in [0,1,2]])\n",
    "vimlts_ff = VimltsLinear(4, activation=lambda x:x,\n",
    "                        num_samples=3, **initializers_kernel)\n",
    "model_ff = tf.keras.Sequential(vimlts_ff, name='vimlts')\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "model_ff.build(input_shape=(None, 2))\n",
    "# x = tf.convert_to_tensor(np.arange(0,5,5, dtype=np.float32).reshape(-1,1))\n",
    "# out_ff = model_ff(x)\n",
    "out_ff = model_ff(tf.ones((2,2)))\n",
    "out_ff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 2, 4), dtype=float32, numpy=\narray([[[-1.0026882 , -0.84923315, -0.14773655, -1.433944  ],\n        [-1.0026882 , -0.84923315, -0.14773655, -1.433944  ]],\n\n       [[-0.9794265 , -1.3599409 , -1.5360392 , -0.9372349 ],\n        [-0.9794265 , -1.3599409 , -1.5360392 , -0.9372349 ]],\n\n       [[-2.3029408 , -1.5292196 , -0.82594895, -0.7008426 ],\n        [-2.3029408 , -1.5292196 , -0.82594895, -0.7008426 ]]],\n      dtype=float32)>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with bias term\n",
    "vimlts_ff = VimltsLinear(4, activation=lambda x:x,\n",
    "                        num_samples=3, **initializers_kernel, **initializers_bias)\n",
    "model_ff = tf.keras.Sequential(vimlts_ff, name='vimlts')\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "model_ff.build(input_shape=(None, 2))\n",
    "# x = tf.convert_to_tensor(np.arange(0,5,5, dtype=np.float32).reshape(-1,1))\n",
    "# out_ff = model_ff(x)\n",
    "out_ff = model_ff(tf.ones((2,2)))\n",
    "out_ff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "res = np.array([[[-1.0026882 , -0.84923315, -0.14773655, -1.433944  ],\n",
    "        [-1.0026882 , -0.84923315, -0.14773655, -1.433944  ]],\n",
    "       [[-0.9794265 , -1.3599409 , -1.5360392 , -0.9372349 ],\n",
    "        [-0.9794265 , -1.3599409 , -1.5360392 , -0.9372349 ]],\n",
    "       [[-2.3029408 , -1.5292196 , -0.82594895, -0.7008426 ],\n",
    "        [-2.3029408 , -1.5292196 , -0.82594895, -0.7008426 ]]])\n",
    "assert np.allclose(out_ff, res), \"Something goes wrong.\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}